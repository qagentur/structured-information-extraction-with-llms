{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured information extraction\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install required Python packages.\n",
    "\n",
    "We use:\n",
    "\n",
    "- `llama-cpp-python` for running the LLM\n",
    "- `instructor` for structured outputs\n",
    "- `polars` for data manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python\n",
    "# !pip install instructor\n",
    "# !pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will run an LLM on Google Colab using Llama.cpp, a highly optimized C++ inference engine for LLMs. It enables running LLMs on laptop-grade hardware.\n",
    "\n",
    "First, we download an open source model from Hugging Face hub. It's **Hermes-3-Llama-3.2-3B**, which is a modified version of Meta's Llama 3.2. It has 3 billion parameters. The developers at Nous Research have quantized it to 4-bit precision. This makes the model faster and smaller, at the cost of some accuracy. We will use it throughout the workshop. Larger models are typically better, so if something doesn't work, it may be because the model is too small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulsimmering/Documents/Py.nosync/gor25-workshop/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_new_context_with_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "\n",
    "# Load model:\n",
    "# https://huggingface.co/NousResearch/Hermes-3-Llama-3.2-3B-GGUF\n",
    "\n",
    "llama = llama_cpp.Llama.from_pretrained(\n",
    "    repo_id=\"NousResearch/Hermes-3-Llama-3.2-3B-GGUF\",\n",
    "    filename=\"Hermes-3-Llama-3.2-3B.Q4_K_M.gguf\",\n",
    "    tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(\n",
    "      \"NousResearch/Hermes-3-Llama-3.2-3B\"\n",
    "    ),\n",
    "    n_gpu_layers=-1,\n",
    "    chat_format=\"chatml\",\n",
    "    n_ctx=8192,\n",
    "    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=2),\n",
    "    logits_all=True,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model with a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news article \"Apple announces new iPhone with revolutionary AI features\" would be classified as Sci/Tech.\n"
     ]
    }
   ],
   "source": [
    "response = llama.create_chat_completion_openai_v1(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classify the following news article as either World, Sports, Business or Sci/Tech: `Apple announces new iPhone with revolutionary AI features`\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct, but it's not structured. We can't easily use it in data analysis. We could ask the model to only return the category, but it may still add extra comments, formatting or other text. Instead, we can force it to always return a valid JSON object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured outputs with instructor\n",
    "\n",
    "Structured information extraction works best with structured outputs. We will use the instructor package for that. This is possible because llama.cpp and instructor both accept the OpenAI API specification, which has become an industry standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import instructor\n",
    "\n",
    "create = instructor.patch(\n",
    "    create=llama.create_chat_completion_openai_v1,\n",
    "    mode=instructor.Mode.JSON_SCHEMA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification\n",
    "\n",
    "Let's do the same text classification task, but now with structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='Sci/Tech'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class NewsCategory(BaseModel):\n",
    "    category: Literal[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "\n",
    "news_category = create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Classify this news article into a category: `Apple announces new iPhone with revolutionary AI features`\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=NewsCategory,\n",
    ")\n",
    "\n",
    "print(news_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "\n",
    "Let's try named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities=[Entity(text='John Smith', type='PERSON'), Entity(text='Microsoft', type='ORGANIZATION'), Entity(text='headquarters', type='LOCATION'), Entity(text='Seattle', type='LOCATION'), Entity(text='last Tuesday', type='DATE')]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    text: str\n",
    "    type: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"OTHER\"]\n",
    "    \n",
    "class NamedEntities(BaseModel):\n",
    "    entities: List[Entity]\n",
    "\n",
    "entities = create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Extract named entities from this text: 'John Smith visited Microsoft headquarters in Seattle last Tuesday'\"\n",
    "        }\n",
    "    ],\n",
    "    response_model=NamedEntities,\n",
    ")\n",
    "\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect-based sentiment analysis\n",
    "\n",
    "Let's try aspect-based sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect='cheeseburger' polarity='positive'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Sentiment(BaseModel):\n",
    "    aspect: str\n",
    "    polarity: Literal[\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "\n",
    "sentiment = create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Analyze the following review with aspect-based sentiment analysis: `The cheeseburger was delicious`\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=Sentiment,\n",
    ")\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free form tasks\n",
    "\n",
    "We can ask the LLM to extract any extraction, transformation or other reasoning task. Let's try a prompt to comprehensively analyze a news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language='de' topic='World' topic_detail='German Politics' summary=\"The far-right Alternative for Germany (AfD) has won more votes than ever before in three state elections in eastern Germany in September, despite being classified as right-wing extremist by Germany's domestic intelligence agency.\" entities=[Entity(text='Alternative for Germany', type='ORGANIZATION'), Entity(text='eastern Germany', type='LOCATION'), Entity(text='September', type='DATE'), Entity(text='right-wing extremist', type='OTHER'), Entity(text='Office for the Protection of the Constitution', type='ORGANIZATION'), Entity(text='ARD Deutschlandtrend', type='ORGANIZATION'), Entity(text='infratest-dimap', type='ORGANIZATION'), Entity(text='1321 Germans', type='OTHER'), Entity(text='AfD voters', type='OTHER'), Entity(text='right issues', type='OTHER'), Entity(text='right-wing extremist', type='OTHER'), Entity(text='strong AfD', type='OTHER'), Entity(text='democracy', type='OTHER'), Entity(text='rule of law', type='OTHER'), Entity(text='Germany', type='LOCATION')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydantic import Field\n",
    "\n",
    "class NewsAnalysis(BaseModel):\n",
    "    language: str = Field(description=\"Language of the article as a two letter code\")\n",
    "    topic: Literal[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "    topic_detail: str = Field(description=\"Detailed categorization of the topic\")\n",
    "    summary: str = Field(description=\"One sentence summary of the article\")\n",
    "    entities: List[Entity]\n",
    "\n",
    "article = \"\"\"\n",
    "In the three state elections in eastern Germany in September, the far-right Alternative for Germany (AfD) won more votes than ever before, even though the party adopted particularly extreme positions in those states and has been classified as right-wing extremist by the Office for the Protection of the Constitution, Germany's domestic intelligence agency.\n",
    "However, this does not bother the party's supporters in the least. According to the latest ARD Deutschlandtrend survey, 84% of AfD voters agree with the statement: \"I don't care that the AfD has been labeled partly right-wing extremist, as long as it addresses the right issues.\"\n",
    "For this survey, the pollsters from infratest-dimap questioned a representative sample of 1321 Germans eligible to vote from October 7 to 9.\n",
    "Overall, two-thirds of respondents said they believe that a strong AfD is a danger to democracy and the rule of law in Germany. Many politicians also share this view. However, opinions begin to diverge when it comes to the question of how best to combat the AfD.\n",
    "\"\"\"\n",
    "# Excerpt from: https://www.dw.com/en/germans-divided-over-far-right-afd-ban/a-70465031\n",
    "\n",
    "analysis = create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Analyze the following news article: {article}\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=NewsAnalysis,\n",
    ")\n",
    "\n",
    "print(analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the multi-task approach we can bundle a whole NLP pipeline into a single prompt. However, it makes evaluation and debugging more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot learning\n",
    "\n",
    "We can add examples to the prompt to improve the model's performance. Let's go back to the text classification example and create a system prompt with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a text classifier. You are given a news article and you need to classify it into one of the following categories:\n",
      "World, Sports, Business, Sci/Tech\n",
      "\n",
      "Examples:\n",
      "{'text': 'UN Security Council passes resolution on global peace initiative', 'category': 'World'}\n",
      "{'text': 'Major tech company reports record quarterly earnings', 'category': 'Business'}\n",
      "{'text': 'Scientists develop breakthrough quantum computing technology', 'category': 'Sci/Tech'}\n",
      "{'text': 'Local team wins national championship in dramatic final', 'category': 'Sports'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"text\": \"UN Security Council passes resolution on global peace initiative\",\n",
    "        \"category\": \"World\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Major tech company reports record quarterly earnings\",\n",
    "        \"category\": \"Business\"\n",
    "    }, \n",
    "    {\n",
    "        \"text\": \"Scientists develop breakthrough quantum computing technology\",\n",
    "        \"category\": \"Sci/Tech\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Local team wins national championship in dramatic final\",\n",
    "        \"category\": \"Sports\"\n",
    "    }\n",
    "]\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a text classifier. You are given a news article and you need to classify it into one of the following categories:\n",
    "{\", \".join(categories)}\n",
    "\n",
    "Examples:\n",
    "{\"\\n\".join(str(example) for example in examples)}\n",
    "\"\"\"\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the system prompt to classify another news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='Business'\n"
     ]
    }
   ],
   "source": [
    "news_category = create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Breakthrough AI technology for market research revealed at GOR 25\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=NewsCategory,\n",
    ")\n",
    "\n",
    "print(news_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can evaluate the performance of the model by comparing the predicted category to the true category. Here, we can run the experiment with the AG News dataset. The dataset contains 127,600 news articles classified into 4 categories: World, Sports, Business, and Science/Technology. Let's see how well the model performs on this dataset. It's available on Hugging Face: <https://huggingface.co/datasets/fancyzhx/ag_news>. We download it as a polars DataFrame. Polars is similar to pandas, but faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "agnews_test = pl.read_parquet('hf://datasets/fancyzhx/ag_news/' + splits['train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text labels are easier to work with than a category ID number, because they are more descriptive and help the LLM understand the task. We can map the category ID to a text label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬───────┬──────────┐\n",
      "│ text                            ┆ label ┆ category │\n",
      "│ ---                             ┆ ---   ┆ ---      │\n",
      "│ str                             ┆ i64   ┆ str      │\n",
      "╞═════════════════════════════════╪═══════╪══════════╡\n",
      "│ Wall St. Bears Claw Back Into … ┆ 2     ┆ Business │\n",
      "│ Carlyle Looks Toward Commercia… ┆ 2     ┆ Business │\n",
      "│ Oil and Economy Cloud Stocks' … ┆ 2     ┆ Business │\n",
      "│ Iraq Halts Oil Exports from Ma… ┆ 2     ┆ Business │\n",
      "│ Oil prices soar to all-time re… ┆ 2     ┆ Business │\n",
      "└─────────────────────────────────┴───────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "agnews_test = agnews_test.with_columns(pl.col('label').replace_strict(label_map).alias('category'))\n",
    "\n",
    "print(agnews_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model on a subset of the test set. Running it on the entire dataset would take a long time in a Colab notebook. If this were running on a powerful GPU, it would be done quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnews_test_sample = agnews_test.sample(n=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 100/100 [00:58<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_response(text):\n",
    "    return create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        response_model=NewsCategory,\n",
    "    )\n",
    "\n",
    "responses = [\n",
    "    get_response(text) for text in tqdm(agnews_test_sample['text'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = [response.category for response in responses]\n",
    "expected_labels = agnews_test_sample['category'].to_list()\n",
    "\n",
    "accuracy = sum(\n",
    "    pred == exp\n",
    "    for pred, exp in zip(predicted_labels, expected_labels)\n",
    ") / len(expected_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is a start but not satisfactory. This highlights the importance of evaluation. If we had just looked at a few examples, we might have falsely concluded that the model is always correct.\n",
    "\n",
    "There are many ways to improve the model's performance:\n",
    "\n",
    "- Describe the different labels in more detail\n",
    "- Add more examples\n",
    "- Use prompting techniques, such as Chain of Thought\n",
    "- Use a higher precision model (less quantization)\n",
    "- Use a larger model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
