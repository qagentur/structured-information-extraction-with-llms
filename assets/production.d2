# Information extraction with LLMs
direction: right

classes: {
  node: {
    style: {
      font-size: 24
    }
  }
  edge: {
    style: {
      font-size: 24
      font-color: black
    }
  }
}

vars: {
  d2-config: {
    layout-engine: dagre
    theme-id: 300
  }
}

Input: {
  label: Input data
  class: node
  shape: cylinder
}

Model Store: {
  class: node
  shape: rectangle
}

LLM: {
  class: node
  shape: rectangle
  icon: https://icons.terrastruct.com/aws%2FMachine%20Learning%2FAmazon-SageMaker_Model_light-bg.svg
}

Inference: {
  class: node
  label: Inference server
  shape: rectangle
}

Client: {
  class: node
  shape: rectangle
}

Versioned config: {
  Few-shot examples: {
    class: node
    shape: rectangle
    style: {
      multiple: true
    }
  }

  Prompt: {
    class: node
    shape: rectangle
  }

  Output Schema: {
    class: node
    shape: rectangle
  }

  Settings: |md
    ## Settings
    - model_id
    - temperature
    - other settings
  |
}

Structured Output: {
  class: node
  shape: rectangle
}

Monitoring: {
  class: node
  label: Monitoring system
  shape: rectangle
}

# Define connections
Input -> Client: {
  class: edge
}
Versioned config -> Client: {
  class: edge
}
Client <-> Inference: {
  class: edge
  label: make parallel requests
}
Model Store -> LLM: {
  class: edge
  label: provide model
}
Inference <-> LLM: {
  class: edge
  label: run
}
Inference -> Structured Output: {
  class: edge
}
Inference -> Monitoring: {
  class: edge
  label: log performance & errors
}
Client -> Monitoring: {
  class: edge
  label: log inputs and outputs
}
