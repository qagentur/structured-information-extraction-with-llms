# Information extraction with LLMs
direction: right

classes: {
  node: {
    style: {
      font-size: 36
    }
  }
  edge: {
    style: {
      font-size: 36
      font-color: black
    }
  }
}

vars: {
  d2-config: {
    layout-engine: elk
    theme-id: 300
  }
}

Input: {
  label: Input data
  class: node
  shape: cylinder
}

Model Store: {
  class: node
  shape: rectangle
}

LLM: {
  class: node
  shape: rectangle
}

Inference: {
  class: node
  label: Inference server
  shape: rectangle
}

Client: {
  class: node
  shape: rectangle
}

Versioned config: {
  Few-shot examples: {
    class: node
    shape: rectangle
    style: {
      multiple: true
    }
  }

  Prompt: {
    class: node
    shape: rectangle
  }

  Output Schema: {
    class: node
    shape: rectangle
  }

  Settings: {
    class: node
    shape: rectangle
    label: "Settings"

    Model ID: {
      class: node
      shape: rectangle
    }
    Temperature: {
      class: node
      shape: rectangle
    }
    Other: {
      class: node
      shape: rectangle
    }
  }
}

Structured Output: {
  class: node
  shape: rectangle
}

Monitoring: {
  class: node
  label: Monitoring system
  shape: rectangle
}

# Define connections
Input -> Client: {
  class: edge
}
Versioned config -> Client: {
  class: edge
}
Client <-> Inference: {
  class: edge
  label: make parallel requests
}
Model Store -> LLM: {
  class: edge
  label: provide model
}
Inference <-> LLM: {
  class: edge
  label: run
}
Inference -> Structured Output: {
  class: edge
}
Inference -> Monitoring: {
  class: edge
  label: log performance & errors
}
Client -> Monitoring: {
  class: edge
  label: log inputs and outputs
}
