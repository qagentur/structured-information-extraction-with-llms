---
title: "GOR 2025 Workshop: Structured Information Extraction with LLMs"
author: "Paul Simmering"
institute: "Q Agentur für Forschung"
format:
    revealjs:
        theme: default
        number-sections: true
        slide-number: true
        highlight-style: monokai
        footer: "Structured Information Extraction with LLMs"
        logo: logo.png
---

# Introduction

## Agenda

TODO: update timings

| Time | Topic |
| ---- | ----- |
| 10:00 | Introduction |
| 10:15 | Setting up the environment |
| 10:30 | Notebook + Exercises |
| 11:30 | Break |
| 11:45 | Comparison of methods |
| 12:00 | Discussion: Classic NLP vs LLMs |
| 12:15 | Models and tools |
| 12:30 | Wrap-up |

## About Q

- I'm a Senior Data Scientist at Q Agentur für Forschung
- Market research agency from Mannheim
- Classic market research + data science
- Using language models since 2019 for social media and review analysis

This workshop builds on the GOR24 presentation "Where do LLMs fit in NLP pipelines?" by my colleague Dr. Paavo Huoviala and me.

## Goals for today

1. Get hands-on experience with structured information extraction
2. Get an overview of available models and tools
3. Learn about evaluation, efficiency and limitations
4. Share experiences and use cases

## Workshop Organization

- Run the examples in the Colab notebook
- Slides and code is available on GitHub: <qagentur.github.com/structured-extraction-workshop>
- Ask questions at any time
- Use my example cases or bring your own

## Google Colab

- We will use a Jupyter notebook, please run it either in Google Colab or on your own computer
- It's a free cloud environment to run Python notebooks
- Please open it now and log in with your Google account
- Add a T4 GPU
- Run the first two cells to install the necessary packages and download the model

Link to the Colab:
<https://colab.research.google.com/drive/1vzlZa2C64KTN4ogROmEADDKC5OKx1aCf#scrollTo=FjcVTAKjR7EA>

# LLMs and structured output

## Output types

:::: {.columns}

::: {.column width="50%"}

**Structured output**

- Text classification
- Named entity recognition
- Sentiment analysis
- Entity linking
- Part of speech tagging

:::

::: {.column width="50%"}

**Unstructured output**

- Chat
- Summarization
- Creative writing
- Translation
- Code generation

:::
::::

## Why use an LLM for structured output?

- Just a few examples are needed
- Do multiple tasks with the same model
- Do multiple tasks at once
- Analyze text, images and audio in the same way
- Easy to use via an API

# Demo and exercises

## Notebook

- Let's run the notebook together
- I'll add more background information and alternatives later

## Exercise 1: Prompt the model

Run the notebook cells and see if you receive an answer from the model. Can you make it answer with just the name of the news category? What could go wrong?

## Exercise 2: Structured output

Modify the examples and play with them. Some ideas:

- add a new news category
- add a new type of entity
- extract the intensity of the sentiment
- find a case where the model fails to give the right answer

## Exercise 3: Multi task prompt

Modify the NewsAnalysis class to extract more information from the news text. Find other news texts on the web and see if the model can extract the information.

## Exercise 4: Few shot learning

Add another news category and an example for it. Test your classifier with some examples.

## Exercise 5: Evaluation

Can you increase the accuracy by adding instructions or more examples?

# Classic NLP vs LLMs

When would you use classic NLP methods over LLMs?

Options:

- Manual prompting
- Automated prompt engineering
- Fine-tuning SLM
- Fine-tuning LLM

When to use which one?

## Benchmarks

TODO: Add benchmarks

## Discussion: When to use which method?

## Best SLMs

- **ModernBERT** is a strong choice for many structured NLP tasks, especially text classification.
- **GliNER** is a strong choice for zero shot named entity recognition.

# Models and tools

## Alternative models

- Size/Cost vs performance
- Open and closed source choices

TODO: Update with latest model choices close to the workshop date

## Alternatives inference engines

We've used Llama.cpp for the demo, which let us run the models without needing an external API.

- **vLLM** for large scale inference with your own GPUs
- **API providers** like OpenAI, Anthropic, Google, Fireworks.ai, Groq...

Huge differences in cost and performance between models and providers. Use [artificialanalysis.ai](https://artificialanalysis.ai) to compare models and providers.

## Ways to get structured output

By default LLMs can generate any token at any position. To use it in a pipeline, we need to constrain the output.

- **JSON mode**: Demand that the output is always a valid JSON object.
- **Tool calling**: Create a JSON according to a given schema. Can be used as input to a function.
- **Constrained generation**: Limit which tokens can be generated so that the output conforms to a schema.

## Alternatives to instructor

In Python:

- [mirascope](https://github.com/mirascope/mirascope)
- [langchain](https://github.com/langchain-ai/langchain)
- [llama_index](https://github.com/run-llama/llama_index)
- [marvin](https://github.com/prefecthq/marvin)
- [outlines](https://github.com/dottxt-ai/outlines)
- [sglang](https://github.com/sgl-project/sglang)

In R:

- [elmer](https://elmer.tidyverse.org/articles/structured-data.html)

Any programming language: directly specifying JSON schemas for your API calls

# Wrap-up

## Summary

- LLMs are powerful tools for structured output
- There are many open source models and libraries available
- Classic models are still strong and are more efficient
- Fine-tuning is required to get the highest accuracy

## Contact

Website: [teamq.de](https://teamq.de)

Github: [github.com/qagentur](https://github.com/qagentur)

Contact me: <paul.simmering@teamq.de>

We offer market research services and consulting on AI projects.
