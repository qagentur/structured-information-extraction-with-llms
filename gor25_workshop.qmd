---
title: "GOR 2025 Workshop: Structured Information Extraction with LLMs"
author: "Paul Simmering"
institute: "Q Agentur für Forschung"
format:
    revealjs:
        theme: default
        number-sections: true
        slide-number: true
        highlight-style: monokai
        footer: "Structured Information Extraction with LLMs"
        logo: logo.png
---

# Introduction

## Agenda

| Time | Topic |
| ---- | ----- |
| 10:00 | Introduction |
| 10:15 | Structured output tasks |
| 10:30 | Learning from examples |
| 10:45 | Evaluation |
| 11:00 | Efficiency |
| 11:15 | Break |
| 11:30 | Exercise 1 |
| 11:45 | Exercise 2 |

## About Q

- I'm a Senior Data Scientist at Q Agentur für Forschung
- Market research agency from Mannheim
- Classic market research + data science
- Using language models since 2019 for social media and review analysis

This workshop builds on the GOR24 presentation "Where do LLMs fit in NLP pipelines?" by my colleague Dr. Paavo Huoviala and me.

## Goals for today

1. Get hands-on experience with structured information extraction
2. Get an overview of available models and tools
3. Learn about evaluation, efficiency and limitations
4. Share experiences and use cases

## Workshop Organization

- Slides and example code is available on GitHub: <qagentur.github.com/structured-extraction-workshop>
- Please ask questions at any time
- 4 exercises, use my example cases or bring your own
- 2 discussions

## Google Colab

- We will use Google Colab for the workshop
- It's free and runs in the browser

Link to the Colab:
<https://colab.research.google.com/drive/1vzlZa2C64KTN4ogROmEADDKC5OKx1aCf#scrollTo=FjcVTAKjR7EA>

# LLMs and structured output

## Structured output tasks

- Text classification
- Named entity recognition
- Sentiment analysis
- Relation extraction
- Summarization
- Question answering

## Why use an LLM for structured output?

- Just a few examples are needed
- Do multiple tasks with the same model
- Do multiple tasks at once
- Analyze text, images and audio in the same way
- Easy to use via an API

## Model choices

- Size/Cost vs performance
- Open and closed source choices

TODO: Update with latest model choices close to the workshop date

## Why is structured output hard?

- By default LLMs can generate any token at any position
- It can return unparsable output

## JSON mode

Demand that the output is always a valid JSON object.

## Function calling

Specify the output format as a JSON schema.

## Constrained generation

Limit which tokens can be generated so that the output conforms to a schema.

## instructor and Pydantic

Instructor is an open source Python package for structured output tasks with LLMs. It builds on Pydantic.

```python
import instructor
from pydantic import BaseModel
from openai import OpenAI

class UserInfo(BaseModel):
    name: str
    age: int

client = instructor.from_openai(OpenAI())

user_info = client.chat.completions.create(
    model="gpt-4o-mini",
    response_model=UserInfo,
    messages=[{"role": "user", "content": "John Doe is 30 years old."}],
)

print(user_info.name) # John Doe
print(user_info.age) # 30
```

## Alternatives to instructor

In Python:

- [mirascope](https://github.com/mirascope/mirascope)
- [langchain](https://github.com/langchain-ai/langchain)
- [llama_index](https://github.com/run-llama/llama_index)
- [marvin](https://github.com/prefecthq/marvin)
- [outlines](https://github.com/dottxt-ai/outlines)
- [sglang](https://github.com/sgl-project/sglang)

In R:

- [elmer](https://elmer.tidyverse.org/articles/structured-data.html)

Any programming language: directly specifying JSON schemas for your API calls

## Exercise 1: Extracting structured information

Pick a structured output task and try to solve it with your language model provider

- Use a Pydantic model to describe the desired output
- Find a case where the model fails to give the right answer

# Learning from examples

## Few-shot learning

Give a few examples of inputs and their outputs in the prompt for in-context learning.

## Exercise 2: Few-shot learning

Add examples to your structured output task

- Do the outputs change?
- How many examples are needed?

## Fine-tuning

Adjust the model's weights to improve performance on a specific task. Requires examples.

## Discussion: Fine-tuning vs few-shot learning

When would you use fine-tuning over few-shot learning?

# Evaluation

## Metrics

- Accuracy
- Precision, recall, F1
- Human evaluation
- LLM as judge

## Exercise 3: Evaluation

Evaluate your model on a test set.

- compare different models
- compare different few-shot examples

## Discussion: Classic NLP vs LLMs

When would you use classic NLP methods over LLMs?

# Efficiency

## Prompt caching

Cache the repeated parts of your prompts so the LLM doesn't have to recompute them every time.

## Request caching

Cache the answer to your API calls so you don't have to pay for the same answer twice.

## Cost control

Check the cost of your API calls and estimate what a batch of calls will cost.

## Parallelization

Run multiple API calls in parallel to speed up your pipeline.

## Exercise 4: Efficiency

Optimize your pipeline for cost and speed.

- check the cost of your API calls
- parallelize your API calls
- cache your prompts

## Contact

Website: [teamq.de](https://teamq.de)

Github: [github.com/qagentur](https://github.com/qagentur)

Contact me: <paul.simmering@teamq.de>

We offer market research services and consulting on AI projects.
