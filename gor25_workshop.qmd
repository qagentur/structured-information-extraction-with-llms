---
title: "GOR 2025 Workshop: Structured Information Extraction with LLMs"
author: "Paul Simmering"
institute: "Q Agentur f√ºr Forschung"
format:
    revealjs:
        theme: default
        number-sections: true
        slide-number: true
        highlight-style: monokai
        footer: "Structured Information Extraction with LLMs"
        logo: assets/logo.png
---

# Introduction

## About

- I'm a Senior Data Scientist at Q Agentur f√ºr Forschung
- Market research agency from Mannheim
- Classic market research + data science
- Using language models since 2019 for social media and review analysis

This workshop builds on the GOR24 presentation "Where do LLMs fit in NLP pipelines?" by my colleague Dr. Paavo Huoviala and me.

## Goals for this workshop

1. Get hands-on experience with structured information extraction
2. Get an overview of available models and tools
3. Learn about evaluation, efficiency and limitations
4. Share experiences and use cases

## Agenda 10:00 - 12:30

| Time | Topic |
| ---- | ----- |
| 10:00 | Introduction & overview |
| 10:15 | Setting up the environment |
| 10:35 | Notebook and exercises |
| 11:35 | Comparison of methods |
| 11:45 | Discussion: Classic NLP vs LLMs |
| 12:00 | Models, tools and best practices |
| 12:15 | Q&A and wrap-up |

Feel free to take a break during the exercises.

## Workshop organization

- Ask questions at any time
- Use my example cases or bring your own
- Slides and code are available on GitHub: <https://github.com/qagentur/structured-extraction-workshop>

## Quick survey

::: {.incremental}

- Have you used a chat assistant, like ChatGPT?
- Do you know Python?
- Have you used an LLM API?
- Have you used tool calling or structured output?

:::

# LLMs and structured output

## Types of language model outputs

:::: {.columns}

::: {.column width="50%"}

**Unstructured output**

- Chat
- Creative writing
- Code generation
- Summarization
- Translation
- Reasoning

:::


::: {.column width="50%"}

<div style="background-color: #03FFAB; color: white; border-radius: 5px; color: black; padding: 10px;">

**Structured output**

- Text classification
- Named entity recognition
- Attribute & relation extraction
- Sentiment analysis
- Entity linking
- Part of speech tagging

</div>

:::

::::

## Information extraction with LLMs

![Basic information extraction pipeline](assets/information_extraction.svg)

## Why use an LLM for structured information extraction?

- Great amount of built-in knowledge
- Lack of labeled examples to train a classic model
- Extreme flexibility:
  - One model for multiple tasks
  - Multiple tasks at once
  - Analyze text, images and audio in the same way
- Easy to use via an API

# Notebook and exercises

## Jupyter notebook setup

- Use Colab (easier, requires Google account) or your own computer
- Colab is a free cloud environment to run Python notebooks
- Use a T4 GPU on Colab
- Run the first few cells to install the necessary packages and download the model

Link to the notebook on Colab:
<https://colab.research.google.com/drive/1vzlZa2C64KTN4ogROmEADDKC5OKx1aCf#scrollTo=FjcVTAKjR7EA>

## Notebook

- Let's run the notebook together
- I'll add more background information and alternatives later
- Use the AG News dataset at <https://huggingface.co/datasets/fancyzhx/ag_news>

## Exercise 1: Prompt the model

Run the notebook cells and see if you receive an answer from the model. 

- Can you make it answer with just the name of the news category?
- What could go wrong?

## Exercise 2: Structured output

Modify the examples and tinker with them. Some ideas:

- Add a new news category
- Add a new type of entity
- Extract the intensity of the sentiment
- Find a case where the model fails to give the right answer
- Implement chain-of-thought:
  - Generate a free form answer
  - Extract the structured output from the answer

## Exercise 3: Multi task prompt

Modify the NewsAnalysis class to extract more information from the news text.

Find other long news texts on the web and see if the model can extract the information.

## Exercise 4: Few shot learning

Add another news category and an example for it. Test your classifier with some examples.

## Exercise 5: Eval & optimization

Let's have a little **competition** and **see how high we can get the accuracy**. Everything is allowed, except for changing the model or using information from the test set. Collaboration is encouraged.

Competiting is optional ‚Äî if you'd rather just try out things at your own pace, that's fine too üòâ

## Boosting accuracy üöÄ

- **More examples**: Add few-shot examples
- **Better definitions**: Detailed definitions of each category
- **Chain-of-thought**: Add an intermediate step where the model thinks out loud
- **Self-consistency**: Ask the model to check its own answer
- **Binary classification**: Ask "is it this category?" separately for each category

## Exercise 5: Winners

::: {.incremental}

- Did a team reach **100%** accuracy?
- Did a team reach **95%** accuracy?
- Did a team reach **90%** accuracy?
- Did a team reach **85%** accuracy?
- Did a team reach **80%** accuracy?
- Did a team reach **75%** accuracy?
- Did a team reach **70%** accuracy?
- Did a team reach **65%** accuracy?

:::

# Classic NLP vs LLMs

## Fine-tuning still beats prompting

![Benchmarks](assets/benchmarks.png)

## Benchmark data sources

![Benchmark data sources](assets/benchmark_sources.png)

## Discussion: When to use which method?

- Manual prompting
- Automated prompt engineering
- Fine-tuning SLM, e.g. ModernBERT or specialized models like GliNER
- Fine-tuning LLM

# Models, tools and best practices

## Production system overview

![](assets/production.svg)

## Best LLMs for structured information extraction

- Size/Cost vs performance
- Open and closed source choices
- Diminishing returns for larger models past 8B parameters [Zhou et al. 2024](https://arxiv.org/abs/2412.02279)

| Version | Parameters | Tools | Vision |
|--------------|------------|--------|--------|
| Llama 3.1 | 8B, 70B, 405B | ‚úÖ | ‚ùå |
| Llama 3.2 | 1B, 3B, 11B, 90B |‚úÖ | ‚úÖ (11B, 90B) |
| Llama 3.3 | 70B |‚úÖ | ‚ùå |

The Llama family of models from Meta is a solid choice. They are freely available on Hugging Face. Olmo 2, Qwen 2.5, Gemma 2, and Phi-4,

TODO: Update with latest model choices close to the workshop date. Use models listed in the benchmarks and on [Artificialanalysis.ai](https://artificialanalysis.ai).

## Alternatives inference engines

We've used Llama.cpp for the demo, which let us run the models without needing an external API.

- **vLLM** for large scale inference with your own GPUs
- **API providers** like OpenAI, Anthropic, Google, Fireworks.ai, Groq...

Huge differences in cost and performance between models and providers. Use [artificialanalysis.ai](https://artificialanalysis.ai) to compare models and providers.

Considerations: available models, cost, throughput, latency, privacy, security

## Ways to get structured output

By default LLMs can generate any token at any position. To use it in a pipeline, we need to constrain the output.

- **Tool calling**: Provide a JSON schema that the model's answer must conform to. Can be used to let the model call a function. Widely supported.
- **Constrained generation**: Limit which tokens can be generated so that the output conforms to an arbitrary schema. Most efficient, but not as widely supported.
- **JSON mode**: Request that the output is always a valid JSON object. Not recommended.

## Alternative client libraries

:::: {.columns}

::: {.column width="50%"}

In Python:

- [mirascope](https://github.com/mirascope/mirascope): tool calling
- [marvin](https://github.com/prefecthq/marvin): tool calling
- [outlines](https://github.com/dottxt-ai/outlines): constrained generation
- [sglang](https://github.com/sgl-project/sglang): constrained generation

:::
::: {.column width="50%"}

In R:

- [ellmer](https://ellmer.tidyverse.org)

:::
::::

Any programming language: directly specifying JSON schemas for your API calls

## Monitoring systems

![Track every LLM call to identify issues, improve accuracy, and manage costs](monitoring.svg)

Many options, typically open core with commercial addons: [Arize AI](https://arize.com), [Helicone](https://helicone.ai), [Inspect](https://inspect.ai-safety-institute.org.uk), [Langsmith](https://langsmith.com), [openllmetry](https://github.com/traceloop/openllmetry), [W&B Weave](https://wandb.ai/site/weave/) and dozens more.

# Wrap-up

## Summary

- LLMs are powerful tools for structured output
- There are many open source models and libraries available
- Classic models are still strong and are more efficient
- Fine-tuning is required to get the highest accuracy

## Contact

Website: [teamq.de](https://teamq.de)

GitHub: [github.com/qagentur](https://github.com/qagentur)

Email: <paul.simmering@teamq.de>

We offer market research services and AI consulting.
